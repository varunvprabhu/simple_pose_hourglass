{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import h5py\n",
    "import tables\n",
    "from decimal import Decimal\n",
    "import time\n",
    "import functools\n",
    "from functools import reduce\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this ensures the program can use all the gpu resources it can get\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset images are resized to 256x256 and stored in an hdf5 file so that the data pipeline is not throttled \n",
    "#while reading from the disk. \n",
    "\n",
    "hdf5_path = \"../data/image_mpii_data.hdf5\"\n",
    "hdf5_pose_path = \"../data/pose_raw.hdf5\"\n",
    "\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "hdf5_pose_file = tables.open_file(hdf5_pose_path, mode='r')\n",
    "\n",
    "pose_images = hdf5_file.root.pose_images[:,:,:,:]\n",
    "pose_data = hdf5_pose_file.root.pose_stick[:,:,:,:]\n",
    "\n",
    "number_of_images = pose_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(number_of_images) + \" \" + str(pose_data.shape[0]))\n",
    "number_train_images = 25936 #90% of whole set\n",
    "number_test_images = 2800 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "def weight_initializer(weight_input, output_channel_size, filter_size): \n",
    "    \n",
    "    _, rows, columns, input_channel_size = [i.value for i in weight_input.get_shape()]\n",
    "    \n",
    "    weight_shape = [filter_size,filter_size,input_channel_size,output_channel_size]\n",
    "\n",
    "    weight_output = tf.Variable(tf.contrib.layers.xavier_initializer(uniform = False)(weight_shape))\n",
    "        \n",
    "    return weight_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution block \n",
    "def conv2d(block_input, num_filters, filter_size = 1, stride_length = 1): \n",
    "    \n",
    "    init_weights = weight_initializer(block_input, num_filters, filter_size) \n",
    "    strides = [1,stride_length,stride_length,1]\n",
    "    block_output = tf.nn.conv2d(block_input,init_weights,strides,padding='VALID')\n",
    "    \n",
    "    return block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(block_input, num_filters, filter_size = 1, stride_length = 1): \n",
    "    \n",
    "    init_weights = weight_initializer(block_input, num_filters, filter_size) \n",
    "    strides = [1,stride_length,stride_length,1]\n",
    "    \n",
    "    block_output = tf.nn.conv2d(block_input,init_weights,strides,padding='VALID')\n",
    "    normalized = tf.contrib.layers.batch_norm(block_output, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(block_input, num_filters):\n",
    "    norm_1 = tf.contrib.layers.batch_norm(block_input, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    conv_1 = conv2d(norm_1, int(num_filters/2), 1, 1)\n",
    "    norm_2 = tf.contrib.layers.batch_norm(conv_1, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    pad = tf.pad(norm_2, np.array([[0,0],[1,1],[1,1],[0,0]]))\n",
    "    conv_2 = conv2d(pad, int(num_filters/2), 3, 1)\n",
    "    norm_3 = tf.contrib.layers.batch_norm(conv_2, 0.9, epsilon=1e-5, activation_fn = tf.nn.relu)\n",
    "    conv_3 = conv2d(norm_3, int(num_filters), 1, 1)\n",
    "    \n",
    "    return conv_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_layer(block_input, num_filters):\n",
    "    \n",
    "    if (block_input.get_shape()[3] == num_filters):\n",
    "        return block_input\n",
    "    else:\n",
    "        conv = conv2d(block_input, num_filters,1,1)\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(block_input, num_filters):\n",
    "    conv = conv_block(block_input, num_filters)\n",
    "    skip = skip_layer(block_input, num_filters)\n",
    "    \n",
    "    return(tf.add_n([conv,skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourglass_unit(input_data, reduction_factor, num_filters):\n",
    "    up_1 = residual(input_data, num_filters)\n",
    "    low = tf.contrib.layers.max_pool2d(input_data, [2,2],[2,2], 'VALID')\n",
    "    low_1 = residual(low, num_filters)\n",
    "    \n",
    "    if reduction_factor > 0:\n",
    "        low_2 = hourglass_unit(low_1, reduction_factor - 1, num_filters)\n",
    "    else:\n",
    "        low_2 = residual(low_1, num_filters)\n",
    "    \n",
    "    low_3 = residual(low_2, num_filters)\n",
    "    up_sample = tf.image.resize_nearest_neighbor(low_3, tf.shape(low_3)[1:3]*2)\n",
    "    return tf.add_n([up_1, up_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourglass_model(input_data, num_blocks, num_filters, reduction_factor, train_model):\n",
    "    pad_1 = tf.pad(input_data, np.array([[0,0],[2,2],[2,2],[0,0]]))\n",
    "    conv_1 = conv2d(pad_1, 64,6,2)\n",
    "    res_1 = residual(conv_1, 128)\n",
    "    pool_1 = tf.contrib.layers.max_pool2d(res_1, [2,2], [2,2], padding= 'VALID')\n",
    "    res_2 = residual(pool_1, 128)\n",
    "    res_3 = residual(res_2, num_filters)\n",
    "    \n",
    "    x1 = [None] * num_blocks\n",
    "    x2 = [None] * num_blocks\n",
    "    x3 = [None] * num_blocks\n",
    "    x4 = [None] * num_blocks\n",
    "    x5 = [None] * num_blocks\n",
    "    x6 = [None] * num_blocks\n",
    "    sum_all = [None] * num_blocks\n",
    "    \n",
    "    x1[0] = hourglass_unit(res_3, reduction_factor, num_filters)\n",
    "    x2[0] = conv_bn_relu(x1[0], num_filters)\n",
    "    x3[0] = conv2d(x2[0], num_filters, 1, 1)\n",
    "    x4[0] = tf.layers.dropout(x3[0], rate = 0.1, training = train_model)\n",
    "    x5[0] = conv2d(x2[0], num_filters, 1, 1)\n",
    "    x6[0] = conv2d(x5[0], num_filters, 1, 1)\n",
    "    sum_all[0] = tf.add_n([x4[0], x6[0], res_3])\n",
    "    \n",
    "    for i in range(1, num_blocks - 1):\n",
    "        x1[i] = hourglass_unit(sum_all[i-1], reduction_factor, num_filters)\n",
    "        x2[i] = conv_bn_relu(x1[i], num_filters)\n",
    "        x3[i] = conv2d(x2[i], num_filters, 1, 1)\n",
    "        x4[i] = tf.layers.dropout(x3[i], rate = 0.1, training = train_model)\n",
    "        x5[i] = conv2d(x2[i], num_filters, 1, 1)\n",
    "        x6[i] = conv2d(x5[i], num_filters, 1, 1)\n",
    "        sum_all[i] = tf.add_n([x4[i], x6[i], sum_all[i-1]])\n",
    "    \n",
    "    x1[num_blocks - 1] = hourglass_unit(sum_all[num_blocks - 2], reduction_factor, num_filters)\n",
    "    x2[num_blocks - 1] = conv_bn_relu(x1[num_blocks - 1], num_filters)\n",
    "    x4[num_blocks - 1] = tf.layers.dropout(x2[num_blocks - 1], rate = 0.1, training = train_model)\n",
    "    x5[num_blocks - 1] = conv2d(x4[num_blocks - 1], 3, 1, 1)\n",
    "    final_output = tf.image.resize_nearest_neighbor(x5[num_blocks - 1], tf.shape(x5[num_blocks - 1])[1:3]*2)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "pose_img_input = tf.placeholder(tf.float32,shape=(batch_size,256,256,3),name='pose_img_ip')\n",
    "###################################################################################################################\n",
    "\n",
    "pose_img_input = pose_img_input/255.0\n",
    "\n",
    "hg_output = hourglass_model(pose_img_input, 4, 256, 3, False) # true while training, false during inference\n",
    "###################################################################################################################\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "restore_model = True\n",
    "\n",
    "#restore variable values. while saving the model further below, im only saving variable values and not the graph. \n",
    "if(restore_model):\n",
    "    saver =  tf.train.Saver()  \n",
    "    saver.restore(sess,'../models/20180905/hg_12_4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter an image index value see the image and the pose data for it below\n",
    "ind_val = 26736\n",
    "imshow(pose_images[ind_val,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer the pose for the image\n",
    "temp_content = pose_images[ind_val,:,:,:]\n",
    "temp_content = temp_content.reshape((1,256,256,3))\n",
    "temp_outputs = sess.run([hg_output],feed_dict={pose_img_input:temp_content})\n",
    "temp_outputs = np.asarray(temp_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the ground truth pose data for the image\n",
    "imshow(pose_data[ind_val,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display and save the inferred pose\n",
    "gen_img = np.clip(temp_outputs[0,0,:,:,:], 0, 255).astype('uint8')\n",
    "imshow(gen_img)\n",
    "imageio.imwrite(\"../output/hg_\"+ \"_\" + str(ind_val) + \".jpg\", gen_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use your webcam to infer images in real time. tested using a logitech webcam on win10.\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "while cap.isOpened(): \n",
    "    rval, frame = cap.read() \n",
    "    if rval == True:\n",
    "        corrected = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        temp_content = corrected.reshape((1,256,256,3))\n",
    "        \n",
    "        temp_outputs = sess.run([hg_output],feed_dict={pose_img_input:temp_content})\n",
    "        temp_outputs = np.asarray(temp_outputs)\n",
    "        \n",
    "        final_img = np.clip(temp_outputs[0,0,:,:,:], 0, 255).astype('uint8')\n",
    "        final_img = cv2.resize(final_img, (512, 512), interpolation=cv2.INTER_CUBIC) \n",
    "        cv2.imshow('image',final_img)\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use an existing video clip to capture frames and convert those frames to pose predictions.\n",
    "#the code below doesnt convert the predictions to a movie automatically. you decide what you want to do with the frames\n",
    "\n",
    "cap = cv2.VideoCapture(\"E:/projects/pose/data/test_5.mp4\") # use your own video clip here\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "i = 0\n",
    "while cap.isOpened(): \n",
    "    rval, frame = cap.read() \n",
    "    if rval == True:\n",
    "        frame = cv2.transpose(frame)\n",
    "        frame = cv2.flip(frame, 0)\n",
    "        corrected = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        temp_content = corrected.reshape((1,256,256,3))\n",
    "        \n",
    "        temp_outputs = sess.run([hg_output],feed_dict={pose_img_input:temp_content})\n",
    "        temp_outputs = np.asarray(temp_outputs)\n",
    "        \n",
    "        final_img = np.clip(temp_outputs[0,0,:,:,:], 0, 255).astype('uint8') \n",
    "        cv2.imshow('image',final_img)\n",
    "        imageio.imwrite(\"../output/video/test_5_\" + str(i) + \".jpg\", final_img)\n",
    "        i = i + 1\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
